# Apache Spark

This section focuses on Apache Spark concepts, internals, and practical engineering patterns for running Spark reliably at scale.

## What this section is for

- Understanding Spark fundamentals (RDDs, DataFrames/Datasets, Spark SQL)
- Performance tuning (partitioning, shuffles, AQE, caching, joins)
- Reliability and operations (resource configs, debugging failures, monitoring)
- File formats and table layouts (Parquet/ORC, partitioning strategies)
- Deployments (local, standalone, YARN, Kubernetes)
